{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EngToFra.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aflah02/English-to-French-Seq2Seq-KerasNLP/blob/main/EngToFra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U9kS9nv0Ann",
        "outputId": "252e64f5-cd4b-47ed-c5b5-314e9f5d7e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 511.7 MB 6.3 kB/s \n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 4.5 kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 53.4 MB/s \n",
            "\u001b[?25h  Building wheel for keras-nlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/aflah02/keras-nlp.git -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Introduction\n",
        "\n",
        "KerasNLP provides lots of building blocks for NLP (model layers, tokenizers, metrics, etc.) and\n",
        "makes it convenient to construct NLP pipelines on the fly.\n",
        "\n",
        "In this tutorial we'll use KerasNLP's `UnicodeTokenizer` to train a sequence-to-sequence Transformer model\n",
        "on English-to-French translation. This example draws inspiration from \n",
        "[Character-level recurrent sequence-to-sequence model example](https://keras.io/examples/nlp/lstm_seq2seq/)\n",
        "by [fchollet](https://twitter.com/fchollet) and uses the same dataset and \n",
        "__Abheest's Guide Here__ and uses the same model architecture and decoding code.\n",
        "\n",
        "This tutorial broadly covers the following:\n",
        "- Tokenization using `keras_nlp.tokenizers.UnicodeCharacterTokenizer` to obtain \n",
        "character level tokens.\n",
        "- A sequence-to-sequence transformer model using KerasNLP's\n",
        "`keras_nlp.layers.TransformerEncoder`, `keras_nlp.layers.TransformerDecoder` and\n",
        "`keras_nlp.layers.TokenAndPositionEmbedding` layers\n",
        "- Utilizes `keras_nlp.utils.greedy_search` ultility to translate text at runtime\n",
        "which implements the Greedy Search Decoding algorithm.\n",
        "\n",
        "This tutorial will be pretty useful and will be a good starting point for learning about KerasNLP and how to \n",
        "incorporate it into your own NLP pipelines.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Setup\n",
        "\n",
        "Importing neccessary libraries\n",
        "\"\"\"\n",
        "\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\"\"\"\n",
        "## Configuration\n",
        "\"\"\"\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "\n",
        "EMBED_DIM = 256\n",
        "INTERMEDIATE_DIM = 2048\n",
        "NUM_HEADS = 8\n",
        "\n",
        "num_samples = 20000\n",
        "data_path = \"fra.txt\"\n",
        "\n",
        "\"\"\"\n",
        "## Downloading the dataset\n",
        "\"\"\"\n",
        "\n",
        "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!!unzip fra-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEkpO0kEQLL6",
        "outputId": "7426bf6e-08e4-433f-8ff1-16e96b9eb7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Archive:  fra-eng.zip',\n",
              " '  inflating: _about.txt              ',\n",
              " '  inflating: fra.txt                 ']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Parsing the Data\n",
        "\n",
        "Each line contains an English sentence and its corresponding French translation.\n",
        "In our setting we treat the English sentence as the *source sequence* and French \n",
        "sentence as our *target sequence*. Upon splitting on the tab character a third\n",
        "entry also pops up but that is unused and hence ignored\n",
        "\"\"\"\n",
        "\n",
        "with open(data_path) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "eng_fra_pairs = []\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    eng, fra, _ = line.split(\"\\t\")\n",
        "    eng_fra_pairs.append((eng, fra))\n",
        "\n",
        "\"\"\"\n",
        "Let's take a look at some random pairs presented in the data.\n",
        "\"\"\"\n",
        "\n",
        "for _ in range(5):\n",
        "    print(random.choice(eng_fra_pairs))\n",
        "\n",
        "\"\"\"\n",
        "At this point we have a huge chunk of data that we can use to train our model however\n",
        "we'll need to split it into training, validation and test data to ensure that our\n",
        "model is able to generalize well.\n",
        "\"\"\"\n",
        "\n",
        "random.shuffle(eng_fra_pairs)\n",
        "num_val_samples = int(0.1 * len(eng_fra_pairs))\n",
        "num_train_samples = len(eng_fra_pairs) - 2 * num_val_samples\n",
        "train_pairs = eng_fra_pairs[:num_train_samples]\n",
        "val_pairs = eng_fra_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = eng_fra_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"Total Samples: {len(eng_fra_pairs)}\")\n",
        "print(f\"Training Samples: {len(train_pairs)}\")\n",
        "print(f\"Validation Samples: {len(val_pairs)}\")\n",
        "print(f\"Testing Samples: {len(test_pairs)}\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## Tokenizing the Data\n",
        "\n",
        "Since the UnicodeTokenizer is a vocabulary free tokenizer which tokenizes text as \n",
        "unicode characters codepoints it makes our job easy as we just need to pass it the\n",
        "text to be tokenized.\n",
        "This also lowercases the text by default before tokenizing.\n",
        "\n",
        "We also compute the MAX_SEQUENCE_LENGTH in our dataset \n",
        "Since we also need something which is analogous to a a VOCAB_SIZE for our model \n",
        "we use the max unicode value present in our English and French characters for \n",
        "the same as all other character tokens lie in the range \n",
        "[0, Max_Unicode_Value_For_Language]\n",
        "\"\"\"\n",
        "\n",
        "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "\n",
        "fra_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = max(max([len(i) for i in eng_samples]), max([len(i) for i in fra_samples]))\n",
        "\n",
        "print(\"MAX_SEQUENCE_LENGTH\", MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "eng_vocab_set = set([])\n",
        "for i in eng_samples:\n",
        "  eng_vocab_set = eng_vocab_set.union(set(list(i)))\n",
        "ENG_VOCAB_SIZE = max([ord(i) for i in eng_vocab_set])+1\n",
        "\n",
        "print(\"ENG_VOCAB_SIZE\", ENG_VOCAB_SIZE)\n",
        "\n",
        "fra_vocab_set = set([])\n",
        "for i in fra_samples:\n",
        "  fra_vocab_set = fra_vocab_set.union(set(list(i)))\n",
        "FRA_VOCAB_SIZE = max([ord(i) for i in fra_vocab_set])+1\n",
        "\n",
        "print(\"FRA_VOCAB_SIZE\", FRA_VOCAB_SIZE)\n",
        "\n",
        "\"\"\"\n",
        "Now, let's define the tokenizers. We will use the vocabularies obtained above as\n",
        "input to the tokenizers.\n",
        "\"\"\"\n",
        "\n",
        "tokenizer = keras_nlp.tokenizers.UnicodeCharacterTokenizer()\n",
        "\n",
        "\"\"\"\n",
        "Let's try to test tokenization and detokenization to make sure it works.\n",
        "We set a random seed for reproducibility of the examples\n",
        "\"\"\"\n",
        "random.seed(30)\n",
        "\n",
        "random_eng_ex = random.choice(eng_samples)\n",
        "random_eng_ex_tokens = tokenizer.tokenize(random_eng_ex)\n",
        "print(\"English sentence: \", random_eng_ex)\n",
        "print(\"Tokens: \", random_eng_ex_tokens)\n",
        "print(\"Recovered text after detokenizing: \", tokenizer.detokenize(random_eng_ex_tokens))\n",
        "\n",
        "print()\n",
        "\n",
        "random_fra_ex = random.choice(fra_samples)\n",
        "random_fra_ex_tokens = tokenizer.tokenize(random_fra_ex)\n",
        "print(\"English sentence: \", random_fra_ex)\n",
        "print(\"Tokens: \", random_fra_ex_tokens)\n",
        "print(\"Recovered text after detokenizing: \", tokenizer.detokenize(random_fra_ex_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--iHN9A1JS4-",
        "outputId": "30e166f7-5182-49ca-f81d-7059e1c87b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Write this down.', 'Mets-le sur papier.')\n",
            "(\"She's a beauty.\", 'Elle est très belle.')\n",
            "(\"It's weird.\", \"C'est bizarre.\")\n",
            "(\"I'm so tired.\", 'Je suis si fatiguée.')\n",
            "('Tom looks nice.', \"Tom a l'air gentil.\")\n",
            "Total Samples: 20000\n",
            "Training Samples: 16000\n",
            "Validation Samples: 2000\n",
            "Testing Samples: 2000\n",
            "MAX_SEQUENCE_LENGTH 53\n",
            "ENG_VOCAB_SIZE 234\n",
            "FRA_VOCAB_SIZE 8240\n",
            "English sentence:  Get away!\n",
            "Tokens:  tf.Tensor([103 101 116  32  97 119  97 121  33], shape=(9,), dtype=int32)\n",
            "Recovered text after detokenizing:  tf.Tensor(b'get away!', shape=(), dtype=string)\n",
            "\n",
            "English sentence:  Skier, c'est sympa.\n",
            "Tokens:  tf.Tensor(\n",
            "[115 107 105 101 114  44  32  99  39 101 115 116  32 115 121 109 112  97\n",
            "  46], shape=(19,), dtype=int32)\n",
            "Recovered text after detokenizing:  tf.Tensor(b\"skier, c'est sympa.\", shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Dataset Preparation\n",
        "\n",
        "We'll need to bring our dataset into a more usable form which can be used for \n",
        "our model\n",
        "\n",
        "As in a normal sequence to sequence setting the model will try to predict \n",
        "N+1th word using information from the source sentence and the previously predicted \n",
        "words (i.e. till the Nth word)\n",
        "\n",
        "We format our dataset into tuples of the form (`inputs`, `target`)\n",
        "\n",
        "- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n",
        "`encoder_inputs` is the tokenized source sentence and `decoder_inputs` is the target sentence \"so far\",\n",
        "that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n",
        "- `target` is the target sentence offset by one step:\n",
        "it provides the next words in the target sentence -- what the model will try to predict.\n",
        "\n",
        "After tokenization we will also add special tokens, -1 acts as the `\"[START]\"` \n",
        "token and 0 acts as the `\"[END]\"` token to the input French sentence. We use 0 \n",
        "as the `\"[END]\"` token as the tensors are padded with 0 to match dimension \n",
        "(max_sequence_length) and they can all be stripped off at the end.\n",
        "\"\"\"\n",
        "\n",
        "def tokenize_and_fix_length_and_add_special_tokens(eng, fra):\n",
        "    batch_size = tf.shape(fra)[0]\n",
        "\n",
        "    eng = tokenizer(eng)\n",
        "    fra = tokenizer(fra)\n",
        "\n",
        "    # Adding Special Tokens\n",
        "    start_tensor = tf.fill((batch_size, 1), -1)\n",
        "    end_tensor = tf.fill((batch_size, 1), 0)\n",
        "    fra = tf.concat([start_tensor, fra, end_tensor], axis=1)\n",
        "\n",
        "    # Setting Tensor Size to Maximum Length Allowed\n",
        "    eng = eng.to_tensor(shape=eng.shape.as_list()[:-1] + [MAX_SEQUENCE_LENGTH])\n",
        "    fra = fra.to_tensor(shape=fra.shape.as_list()[:-1] + [MAX_SEQUENCE_LENGTH+1])\n",
        "\n",
        "    return (\n",
        "        {\"encoder_inputs\": eng, \"decoder_inputs\": fra[:, :-1],},\n",
        "        fra[:, 1:],\n",
        "    )\n",
        "\n",
        "\n",
        "def make_data(pairs):\n",
        "    eng_texts = [pair[0] for pair in pairs]\n",
        "    fra_texts = [pair[1] for pair in pairs]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fra_texts)).batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(tokenize_and_fix_length_and_add_special_tokens, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_data(train_pairs)\n",
        "val_ds = make_data(val_pairs)\n",
        "\n",
        "\"\"\"\n",
        "Let's check our shapes!\n",
        "We have batches of 64 pairs, and all sequences are MAX_SEQUENCE_LENGTH steps long:\n",
        "\"\"\"\n",
        "\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v_hdGkK0DyT",
        "outputId": "ea1ef8a5-e197-498f-d2bd-4e7c1f4adf39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 53)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 53)\n",
            "targets.shape: (64, 53)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Model Architecture\n",
        "\n",
        "We use randomly initialized embeddings layer while we use `keras_nlp.layers.TokenAndPositionEmbedding `\n",
        "layer for getting our position embeddings. We then simply add these 2 embeddings!\n",
        "\n",
        "Our model has an encoder and decoder block present together. The encoder uses \n",
        "`keras_nlp.layers.TransformerEncoder` while the decoder uses\n",
        "`keras_nlp.layers.TransformerDecoder`. The setting is however fairly simple\n",
        "we pass our original english sentence to the Encoder which generate a output. \n",
        "This output along with the character predicted so far are then given to the \n",
        "decoder to product the output at the next time step! We also set `use_causal_mask`\n",
        "to True as we don't want out model to see beyond the already predicted tokens at\n",
        "this stage as then it would use the tokens it needs to predict to predict themselves\n",
        "which is information not available at test time hence this parameter prevents the \n",
        "decoder from peeping into the future! \n",
        "\"\"\"\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=ENG_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        ")(encoder_inputs)\n",
        "\n",
        "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(inputs=x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=FRA_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(decoder_inputs)\n",
        "\n",
        "x = keras_nlp.layers.TransformerDecoder(\n",
        "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
        ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs, use_causal_mask=True,)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = keras.layers.Dense(FRA_VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs,], decoder_outputs,)\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\",\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "## Training our Model\n",
        "\n",
        "We use accuracy as a metric to monitor our training progress. Accuracy might not \n",
        "be the best metric here as other more suitable metrics such as 'BLEU' exist \n",
        "however they are more computationally expensive and will make the training much\n",
        "slower!\n",
        "\"\"\"\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzjYBnx2Jevp",
        "outputId": "e7ecde38-eb08-4211-f271-262c6fe3b92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " token_and_position_embedding (  (None, None, 256)   73472       ['encoder_inputs[0][0]']         \n",
            " TokenAndPositionEmbedding)                                                                       \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Transform  (None, None, 256)   1315072     ['token_and_position_embedding[0]\n",
            " erEncoder)                                                      [0]']                            \n",
            "                                                                                                  \n",
            " model_1 (Functional)           (None, None, 8240)   6738736     ['decoder_inputs[0][0]',         \n",
            "                                                                  'transformer_encoder[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,127,280\n",
            "Trainable params: 8,127,280\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 34s 79ms/step - loss: 0.8922 - accuracy: 0.3455 - val_loss: 0.6870 - val_accuracy: 0.4545\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 18s 72ms/step - loss: 0.6376 - accuracy: 0.4980 - val_loss: 0.5902 - val_accuracy: 0.5260\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 18s 73ms/step - loss: 0.5521 - accuracy: 0.5619 - val_loss: 0.5063 - val_accuracy: 0.5936\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 18s 74ms/step - loss: 0.4841 - accuracy: 0.6140 - val_loss: 0.4705 - val_accuracy: 0.6207\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 19s 75ms/step - loss: 0.4302 - accuracy: 0.6556 - val_loss: 0.4136 - val_accuracy: 0.6682\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 19s 76ms/step - loss: 0.3875 - accuracy: 0.6898 - val_loss: 0.3887 - val_accuracy: 0.6915\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 19s 77ms/step - loss: 0.3524 - accuracy: 0.7166 - val_loss: 0.3696 - val_accuracy: 0.7050\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 19s 76ms/step - loss: 0.3218 - accuracy: 0.7403 - val_loss: 0.3543 - val_accuracy: 0.7207\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 19s 75ms/step - loss: 0.2956 - accuracy: 0.7607 - val_loss: 0.3496 - val_accuracy: 0.7296\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 19s 76ms/step - loss: 0.2716 - accuracy: 0.7787 - val_loss: 0.3403 - val_accuracy: 0.7375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb1900fe3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Analyzing our Outputs\n",
        "\n",
        "Let's checkout how you can swiftly use the model to convert your English Sentences\n",
        "to French\n",
        "We provide the model the tokenized english sentence as well as -1 as the prompt \n",
        "token which was analogous to the START token usually used in such tasks. \n",
        "The model uses these 2 pieces of information to generate a probability distribution \n",
        "over the next possible tokens and we choose the most likely one out of those \n",
        "in a greedy fashion. This is repeated till we hit the end token (0 in our case) \n",
        "or a predetermined maximum length for the output.\n",
        "\n",
        "The above process can be easily performed using the \n",
        "`keras_nlp.utils.greedy_search` present in the offerings of Keras-NLP\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def decode_sequences(input_sentences):\n",
        "    batch_size = tf.shape(input_sentences)[0]\n",
        "\n",
        "    # Tokenize the encoder input.\n",
        "    tokenized = tokenizer(input_sentences)\n",
        "    encoder_input_tokens = tokenized.to_tensor(shape=tokenized.shape.as_list()[:-1] + [MAX_SEQUENCE_LENGTH])\n",
        "    # Define a function that outputs the next token's probability given the\n",
        "    # input sequence.\n",
        "    def token_probability_fn(decoder_input_tokens):\n",
        "        return transformer([encoder_input_tokens, decoder_input_tokens])[:, -1, :]\n",
        "\n",
        "    # Set the prompt to the \"[START]\" token.\n",
        "    prompt = tf.fill((batch_size, 1), -1)\n",
        "\n",
        "    generated_tokens = keras_nlp.utils.greedy_search(\n",
        "        token_probability_fn,\n",
        "        prompt,\n",
        "        max_length=max([len(i) for i in fra_samples]),\n",
        "        end_token_id=0,\n",
        "    )\n",
        "\n",
        "    # Masking the -1 which was given as the initial prompt and removing it\n",
        "    mask = tf.math.equal(generated_tokens,-1)\n",
        "    generated_tokens = tf.boolean_mask(generated_tokens, mask == False)\n",
        "\n",
        "    # Reshaping the retrived tensor after deletion the original shape\n",
        "    generated_tokens = tf.reshape(generated_tokens, [1,tf.shape(generated_tokens)[0]])\n",
        "\n",
        "    generated_sentences = tokenizer.detokenize(generated_tokens)\n",
        "    return generated_sentences\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "\n",
        "for i in range(10):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequences(tf.constant([input_sentence]))\n",
        "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
        "    print(input_sentence)\n",
        "    print(translated)\n",
        "    print()\n",
        "\n",
        "\"\"\"\n",
        "## Conclusions\n",
        "\n",
        "Bear in mind that when our model started training it didn't even know a word is\n",
        "and didn't use any information about word structure directly to learn so it does \n",
        "quite a decent job. Within a few epochs it can clearly form sentences and more training\n",
        "on more data can make it perform much better!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EGCfjTc10Q_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "c3da9c66-0141-4ef3-b3fa-86d41106f300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He found it.\n",
            "il a trouvé inque c'est moi.\n",
            "\n",
            "See you there.\n",
            "à vous devotre votre !\n",
            "\n",
            "I was detained.\n",
            "j'ai été inciné.\n",
            "\n",
            "Everyone agreed.\n",
            "tout le monde a chien.\n",
            "\n",
            "It's outdated.\n",
            "c'est beaucoup de faire.\n",
            "\n",
            "We'll rebuild.\n",
            "nous sommes fini.\n",
            "\n",
            "I nailed it.\n",
            "j'ai besoin d'espare.\n",
            "\n",
            "I can jump.\n",
            "je vous suis joure mettra.\n",
            "\n",
            "Everybody stayed.\n",
            "tout le monde ment sendé.\n",
            "\n",
            "I can't swim.\n",
            "je ne sais pas en moi.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n## Conclusions\\nBear in mind that when our model started training it didn't even know a word is\\nand didn't use any information about word structure directly to learn so it does \\nquite a decent job. Within a few epochs it can clearly form sentences and more training\\non more data can make it perform much better!\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VPdzLTJVoj9M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}